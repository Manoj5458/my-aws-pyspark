{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SgZdv2baiKo"
   },
   "source": [
    "# Pyspark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WGmC9tlWaYEf"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, year\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CustomerPurchaseAnalysis\")\\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"ACCESS_KEY\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"SECRET_KEY\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.metastore.metrics.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"ap-south-1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QS7DfLPR_nf0"
   },
   "source": [
    "# Customer Purchase Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "kvfNc90xAiTl"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "qbkjcDMn9mY0"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load data from s3\n",
    "sales_input = \"s3a://this-is-my-bucket007/sales_data.csv\"\n",
    "sales_df = spark.read.csv(sales_input, header=True, inferSchema=True)\n",
    "sales_df = sales_df.withColumn(\"purchase_date\", col(\"purchase_date\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "EfedmM1IAqys"
   },
   "outputs": [],
   "source": [
    "# Load customer data from DynamoDB\n",
    "response = table.scan()\n",
    "cust_data = response['Items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "V5vHv3I6AtfE"
   },
   "outputs": [],
   "source": [
    "# Convert the DynamoDB items to DataFrame\n",
    "cust_df = spark.createDataFrame(cust_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqYYQwoMBRCz"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "oG-0kgGkAwYE"
   },
   "outputs": [],
   "source": [
    "# Join DataFrames on customer_id, assuming 'id' is the customer ID column in cust_df\n",
    "joined_df = sales_df.join(cust_df, sales_df[\"customer_id\"] == cust_df[\"id\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "JqgU3vM7Bo1D"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "# Define window partitioned by customer_id and ordered by purchase_date\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(\"purchase_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "5LPStm_BB1a0"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lag, avg, unix_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9hUxJrzKBxbj",
    "outputId": "89556a17-c835-4e07-cfbf-2aefffabb11a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------------+--------------------------+\n",
      "|customer_id|avg_purchase_interval_seconds|avg_purchase_interval_days|\n",
      "+-----------+-----------------------------+--------------------------+\n",
      "+-----------+-----------------------------+--------------------------+\n",
      "\n",
      "Customer purchase interval analysis completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate time difference between each transaction (in seconds)\n",
    "interval_df = joined_df.withColumn(\n",
    "    \"previous_purchase_date\", lag(\"purchase_date\").over(window_spec)\n",
    ").withColumn(\n",
    "    \"purchase_interval\",\n",
    "    (unix_timestamp(\"purchase_date\") - unix_timestamp(\"previous_purchase_date\"))\n",
    ")\n",
    "\n",
    "# Calculate average transaction interval per customer (in seconds)\n",
    "avg_interval_df = interval_df.groupBy(\"customer_id\").agg(\n",
    "    avg(\"purchase_interval\").alias(\"avg_purchase_interval_seconds\")\n",
    ")\n",
    "\n",
    "# Convert seconds to days for easier interpretation\n",
    "avg_interval_df = avg_interval_df.withColumn(\n",
    "    \"avg_purchase_interval_days\", col(\"avg_purchase_interval_seconds\") / 86400\n",
    ")\n",
    "\n",
    "# Identify high-engagement customers (e.g., customers with average interval < 30 days)\n",
    "high_engagement_df = avg_interval_df.filter(col(\"avg_purchase_interval_days\") < 30)\n",
    "high_engagement_df.show()\n",
    "\n",
    "print(\"Customer purchase interval analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ligrbX0kB6GT"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
