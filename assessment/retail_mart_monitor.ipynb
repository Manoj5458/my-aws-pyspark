{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Py-9cUe2G2al"
   },
   "source": [
    "# Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RegCsaNreucq"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, expr, year\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MonitorAnalysis\")\\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.11.901\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"ACCESS_KEY\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"SECRET_KEY\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.metastore.metrics.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.io.native.lib.available\", \"false\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .config(\"spark.driver.memory\", \"4g\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.region\", \"ap-south-1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50XYqgxoG6iM"
   },
   "source": [
    "**OS Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssrrJzeJGjMu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = 'ACCESS_KEY'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = 'SECRET_KEY'\n",
    "os.environ['AWS_DEFAULT_REGION'] = 'ap-south-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BveTS6oNHA5N"
   },
   "source": [
    "**Boto Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1NFK4PJGydc"
   },
   "outputs": [],
   "source": [
    "pip install boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwXMea5wG_ws"
   },
   "source": [
    "Boto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw2sQSUTGt2d"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a session with your credentials\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id='ACCESS_KEY',\n",
    "    aws_secret_access_key='SECRET_KEY',\n",
    "    region_name='ap-south-1'  # e.g., 'us-east-1'\n",
    ")\n",
    "\n",
    "dynamodb = session.resource('dynamodb')\n",
    "table = dynamodb.Table('customers')\n",
    "anomaly_table = dynamodb.Table('anomaly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZrS04qS7K4ga"
   },
   "source": [
    "# Retail Mart Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pgELY9unK8XI"
   },
   "outputs": [],
   "source": [
    "response = table.scan()\n",
    "cust_data = response['Items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZyoHZ-IoMrYI"
   },
   "outputs": [],
   "source": [
    "# Convert the DynamoDB items to DataFrame\n",
    "cust_df = spark.createDataFrame(cust_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jPQ7a7SEMvk4"
   },
   "outputs": [],
   "source": [
    "# Load data from s3\n",
    "sales_input = \"s3a://this-is-my-bucket007/sales_data.csv\"\n",
    "sales_df = spark.read.csv(sales_input, header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEp9k0l2NASx",
    "outputId": "7cded7a5-e268-4a11-b5f0-30a90c9c7c00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customer Data from DynamoDB:\n",
      "+-----------+-------------+--------------+\n",
      "|customer_id|customer_name|      inactive|\n",
      "+-----------+-------------+--------------+\n",
      "|       C125|        manoj|{BOOL -> true}|\n",
      "|       C126|          Leo|{BOOL -> true}|\n",
      "|       C123|         John|{BOOL -> true}|\n",
      "|       C124|        brock|{BOOL -> true}|\n",
      "+-----------+-------------+--------------+\n",
      "\n",
      "Transaction Data from S3:\n",
      "+-----------+--------------+----------+-------------+---------------+\n",
      "|customer_id|transaction_id|product_id|purchase_date|purchase_amount|\n",
      "+-----------+--------------+----------+-------------+---------------+\n",
      "|       C123|        TXN001|      P001|   2023-01-05|         150.75|\n",
      "|       C124|        TXN002|      P003|   2023-01-10|          200.5|\n",
      "|       C123|        TXN003|      P002|   2023-02-15|          300.0|\n",
      "|       C125|        TXN004|      P004|   2023-03-10|         120.25|\n",
      "|       C123|        TXN005|      P001|   2023-03-20|          450.0|\n",
      "|       C124|        TXN006|      P003|   2023-03-25|          175.3|\n",
      "|       C126|        TXN007|      P002|   2023-04-01|          620.0|\n",
      "|       C125|        TXN008|      P004|   2023-04-15|         220.75|\n",
      "|       C123|        TXN009|      P002|   2023-05-05|          310.5|\n",
      "|       C124|        TXN010|      P001|   2023-05-12|         140.75|\n",
      "+-----------+--------------+----------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Customer Data from DynamoDB:\")\n",
    "cust_df.show()\n",
    "print(\"Transaction Data from S3:\")\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "N8qQYIwCNGx_"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MpjlyxxsNEnH"
   },
   "outputs": [],
   "source": [
    "# Calculate average spending per transaction for each customer\n",
    "avg_spending_df = (\n",
    "    sales_df.groupBy(\"customer_id\").agg(avg(\"purchase_amount\").alias(\"avg_spending\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tmI3bgTnNMpo"
   },
   "outputs": [],
   "source": [
    "# Join sales data with average spending per customer\n",
    "sales_with_avg_df = sales_df.join(avg_spending_df, on=\"customer_id\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xOdqgNLQNSfQ"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "--0xVUNgNP3I"
   },
   "outputs": [],
   "source": [
    "# Define a threshold (e.g., 1.5x the average spending) to flag anomalies\n",
    "threshold_multiplier = 1.5\n",
    "anomalies_df = sales_with_avg_df.withColumn(\n",
    "    \"is_anomaly\",\n",
    "    when(col(\"purchase_amount\") > (col(\"avg_spending\") * threshold_multiplier), True).otherwise(False)\n",
    ").filter(col(\"is_anomaly\") == True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t0JAdxnGNX8R",
    "outputId": "9e8eccdf-3feb-4804-b914-961144ba8ded"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anamolies DF:\n",
      "+-----------+--------------+----------+-------------+---------------+------------+----------+\n",
      "|customer_id|transaction_id|product_id|purchase_date|purchase_amount|avg_spending|is_anomaly|\n",
      "+-----------+--------------+----------+-------------+---------------+------------+----------+\n",
      "+-----------+--------------+----------+-------------+---------------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Anamolies DF:\")\n",
    "anomalies_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "FC_waTn4NbFH"
   },
   "outputs": [],
   "source": [
    "# Collect anomalies and log them in DynamoDB\n",
    "for row in anomalies_df.collect():\n",
    "    anomaly_table.put_item(\n",
    "        Item={\n",
    "            \"customer_id\": row[\"customer_id\"],\n",
    "            \"transaction_id\": row[\"transaction_id\"],\n",
    "            \"purchase_amount\": row[\"purchase_amount\"],\n",
    "            \"avg_spending\": row[\"avg_spending\"],\n",
    "            \"is_anomaly\": row[\"is_anomaly\"]\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "efFsSvWjNfh4",
    "outputId": "09691345-83f0-4388-a46d-0b05d7e5e11e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalies flagged and logged in DynamoDB.\n"
     ]
    }
   ],
   "source": [
    "print(\"Anomalies flagged and logged in DynamoDB.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XP2z5h83Ng3p"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
